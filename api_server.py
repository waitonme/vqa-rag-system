from fastapi import FastAPI, File, UploadFile, HTTPException, Request, Depends, Header
from fastapi.middleware.cors import CORSMiddleware
from fastapi.exceptions import RequestValidationError
from fastapi.responses import JSONResponse, StreamingResponse
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from pydantic import BaseModel
from typing import List, Optional, Dict, Any
import tempfile
import os
import fitz
from datetime import datetime
import json
import uuid
import time
import asyncio
import numpy as np
from utils import simple_parse
from models_client import ModelServerClient, VQARAGModelWrapper
from debug_chunk import print_chunk_debug, detect_debug_command
from io import BytesIO
from PIL import Image
import base64
import requests as req

app = FastAPI(
    title="VQA RAG API Server",
    description="OpenWebUI í˜¸í™˜ PDF ì²˜ë¦¬ ë° RAG API ì„œë²„ (ëª¨ë¸ ì„œë²„ ë¶„ë¦¬)",
    version="2.0.0",
    docs_url="/docs",
    redoc_url="/redoc"
)

# ì¸ì¦ ìŠ¤í‚¤ë§ˆ
security = HTTPBearer()

# ê°„ë‹¨í•œ API í‚¤ ê²€ì¦
VALID_API_KEYS = {"test-api-key", "your-api-key-here"}

def verify_api_key(credentials: HTTPAuthorizationCredentials = Depends(security)):
    """API í‚¤ ê²€ì¦"""
    if credentials.credentials not in VALID_API_KEYS:
        raise HTTPException(status_code=401, detail="Invalid API key")
    return credentials.credentials

# ëª¨ë¸ ì„œë²„ í´ë¼ì´ì–¸íŠ¸
model_client = None

# í˜„ì¬ í™œì„±í™”ëœ ëª¨ë¸ê³¼ íŒŒì¼ ì •ë³´
uploaded_files = {}
file_models = {}

# Validation ì˜¤ë¥˜ í•¸ë“¤ëŸ¬
@app.exception_handler(RequestValidationError)
async def validation_exception_handler(request: Request, exc: RequestValidationError):
    return JSONResponse(
        status_code=422,
        content={"detail": exc.errors(), "body": "Validation error occurred"}
    )

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ìš”ì²­ ë¡œê¹… ë¯¸ë“¤ì›¨ì–´
@app.middleware("http")
async def log_requests(request: Request, call_next):
    start_time = time.time()
    response = await call_next(request)
    process_time = time.time() - start_time
    print(f"â±ï¸ {request.method} {request.url.path} - {response.status_code} ({process_time:.3f}s)")
    return response

class ChatMessage(BaseModel):
    role: str
    content: Optional[Any] = ""
    images: Optional[Any] = None
    image_url: Optional[Dict[str, Any]] = None

class FileReference(BaseModel):
    type: str
    id: str

class ChatRequest(BaseModel):
    model: str
    messages: List[ChatMessage]
    files: Optional[List[FileReference]] = []
    stream: Optional[bool] = False
    temperature: Optional[float] = None
    max_tokens: Optional[int] = None
    top_p: Optional[float] = None
    frequency_penalty: Optional[float] = None
    presence_penalty: Optional[float] = None
    stop: Optional[List[str]] = None
    features: Optional[Dict[str, Any]] = None
    variables: Optional[Dict[str, Any]] = None
    stream_options: Optional[Dict[str, Any]] = None
    seed: Optional[int] = None
    images: Optional[Any] = None
    image_urls: Optional[Any] = None
    
    class Config:
        extra = "allow"

class ChatResponse(BaseModel):
    id: Optional[str] = None
    object: str = "chat.completion"
    created: Optional[int] = None
    model: str
    choices: List[Dict[str, Any]]
    usage: Optional[Dict[str, int]] = None

class ModelInfo(BaseModel):
    id: str
    object: str = "model"
    created: int
    owned_by: str = "vqa-rag"

class FileUploadResponse(BaseModel):
    id: str
    user_id: str = "system"
    hash: Optional[str] = None
    filename: str
    data: Optional[Dict[str, Any]] = None
    meta: Dict[str, Any]
    created_at: int
    updated_at: int

def connect_to_model_server():
    """ëª¨ë¸ ì„œë²„ ì—°ê²° ì´ˆê¸°í™”"""
    global model_client
    
    model_client = ModelServerClient("http://localhost:8001")
    
    if not model_client.is_healthy():
        print("ëª¨ë¸ ì„œë²„ ì—°ê²° ì‹¤íŒ¨! ëª¨ë¸ ì„œë²„ë¥¼ ë¨¼ì € ì‹œì‘í•´ì£¼ì„¸ìš”.")

@app.on_event("startup")
async def startup_event():
    """ì„œë²„ ì‹œì‘ ì‹œ ëª¨ë¸ ì„œë²„ ì—°ê²°"""
    connect_to_model_server()

@app.get("/v1/models")
async def list_models():
    """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ë°˜í™˜"""
    base_models = [
        "text_only_rag", "vqa_rag", "vqa_rag_cag", "async_vqa_rag_cag"
    ]
    
    model_list = []
    for model_id in base_models:
        model_list.append(ModelInfo(
            id=model_id,
            created=int(datetime.now().timestamp())
        ))
    
    return {"data": model_list}

@app.get("/api/models")
async def list_models_api(api_key: str = Depends(verify_api_key)):
    """OpenWebUI í‘œì¤€ ëª¨ë¸ ëª©ë¡ ì—”ë“œí¬ì¸íŠ¸"""
    return await list_models()

@app.get("/api/v1/models")
async def list_models_v1(api_key: str = Depends(verify_api_key)):
    """OpenWebUI v1 ëª¨ë¸ ëª©ë¡ ì—”ë“œí¬ì¸íŠ¸"""
    return await list_models_api(api_key)

async def extract_pdf_content(temp_file_path: str):
    """PDF ë‚´ìš© ì¶”ì¶œ (í…ìŠ¤íŠ¸ì™€ ì´ë¯¸ì§€)"""
    try:
        pdf = fitz.open(temp_file_path)
        simple_texts, simple_images = simple_parse(pdf)
        return pdf, simple_texts, simple_images
        
    except Exception as e:
        raise e

async def create_file_model(file_id: str, pdf, simple_texts: List[str], simple_images: List, clean_filename: str):
    """íŒŒì¼ë³„ ëª¨ë¸ ìƒì„± ë° í…ìŠ¤íŠ¸ ì²­í‚¹"""
    global model_client, file_models
    
    if not model_client or not model_client.is_healthy():
        raise HTTPException(status_code=500, detail="ëª¨ë¸ ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    # íŒŒì¼ë³„ ëª¨ë¸ ë˜í¼ ìƒì„±
    file_model = VQARAGModelWrapper(model_client)
    
    # ëª¨ë¸ ë°ì´í„° ì„¤ì •
    file_model.pdf = pdf
    file_model.texts = simple_texts
    file_model.images = []
    file_model.document_name = clean_filename
    file_model.file_id = file_id
    
    # í…ìŠ¤íŠ¸ ì²­í‚¹
    try:
        # ì „ì²´ í…ìŠ¤íŠ¸ ê²°í•©
        full_text = "\n".join(simple_texts)
        
        # utilsì˜ split_text_into_chunks ì‚¬ìš©
        from utils import split_text_into_chunks
        raw_chunks = split_text_into_chunks(full_text, None)  # tokenizer ì—†ì´ ê°„ë‹¨ ì²­í‚¹
        
        # ë¬¸ì„œëª… íƒœê¹…
        file_model.chunks = [f"[ë¬¸ì„œ: {clean_filename}] {chunk}" for chunk in raw_chunks]
        
        # ì„ë² ë”© ìƒì„±
        file_model.embed_chunks()
        
        return file_model
        
    except Exception as e:
        # í´ë°±: ê°„ë‹¨í•œ ì²­í‚¹
        chunk_size = 1000
        full_text = "\n".join(simple_texts)
        raw_chunks = [full_text[i:i+chunk_size] for i in range(0, len(full_text), chunk_size)]
        file_model.chunks = [f"[ë¬¸ì„œ: {clean_filename}] {chunk}" for chunk in raw_chunks]
        file_model.embed_chunks()
        return file_model

@app.post("/api/v1/files/")
async def upload_file_standard(file: UploadFile = File(...), api_key: str = Depends(verify_api_key)):
    """í‘œì¤€ íŒŒì¼ ì—…ë¡œë“œ ì—”ë“œí¬ì¸íŠ¸"""
    return await _upload_file_logic(file)

@app.post("/v1/files/")
async def upload_file_v1(file: UploadFile = File(...)):
    """v1 íŒŒì¼ ì—…ë¡œë“œ ì—”ë“œí¬ì¸íŠ¸"""
    return await _upload_file_logic(file)

@app.put("/v1/process")
async def process_document_put_v1(request: Request):
    """OpenWebUI PUT ìš”ì²­ ë¬¸ì„œ ì²˜ë¦¬ ì—”ë“œí¬ì¸íŠ¸"""
    try:
        # raw binary data ì½ê¸°
        file_data = await request.body()
        
        # íŒŒì¼ëª… ì¶”ì¶œ
        filename = "document.pdf"
        
        content_disposition = request.headers.get("content-disposition", "")
        if content_disposition:
            import re
            filename_match = re.search(r'filename[*]?=([^;\r\n"]*)', content_disposition)
            if filename_match:
                filename = filename_match.group(1).strip('"\'')
        elif request.headers.get("x-filename"):
            filename = request.headers.get("x-filename")
        elif request.query_params.get("filename"):
            filename = request.query_params.get("filename")
        else:
            from datetime import datetime
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"document_{timestamp}.pdf"
        
        # íŒŒì¼ëª… ì •ë¦¬
        import re
        filename = re.sub(r'[^\w\-_\.]', '_', filename)
        if not filename.endswith('.pdf'):
            filename += '.pdf'
        
        # ì„ì‹œ íŒŒì¼ ìƒì„±
        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as temp_file:
            temp_file.write(file_data)
            temp_file_path = temp_file.name
        
        # UploadFile ê°ì²´ ì‹œë®¬ë ˆì´ì…˜
        class MockUploadFile:
            def __init__(self, file_path, content_type, filename):
                self.filename = filename
                self.content_type = content_type
                self._file_path = file_path
                
            async def read(self):
                with open(self._file_path, 'rb') as f:
                    return f.read()
        
        mock_file = MockUploadFile(temp_file_path, "application/pdf", filename)
        result = await _upload_file_logic(mock_file)
        
        # ë¶„ì„ ìš”ì•½ ìƒì„±
        file_id = result.id
        analysis_summary = f"""ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ:
- ì´ í˜ì´ì§€: {result.data.get('total_pages', 0)}ê°œ
- í…ìŠ¤íŠ¸ ì²­í¬: {result.data.get('text_chunks', 0)}ê°œ
- ì²˜ë¦¬ ìƒíƒœ: {result.data.get('processing_status', 'completed')}

ë¬¸ì„œê°€ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤. ë¬¸ì„œ ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•´ì£¼ì„¸ìš”!"""
        
        return {
            "page_content": analysis_summary,
            "metadata": {
                "filename": result.filename,
                "size": result.meta["size"],
                "content_type": result.meta["content_type"],
                "created_at": result.created_at,
                "file_id": result.id,
                "total_pages": result.data.get('total_pages', 0),
                "total_chunks": result.data.get('total_chunks', 0),
                "processing_status": result.data.get('processing_status', 'completed')
            }
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Document processing failed: {str(e)}")

async def _upload_file_logic(file: UploadFile):
    """íŒŒì¼ ì—…ë¡œë“œ ê³µí†µ ë¡œì§"""
    global file_models, uploaded_files
    
    if not file.filename.endswith('.pdf'):
        raise HTTPException(status_code=400, detail="PDF íŒŒì¼ë§Œ ì—…ë¡œë“œ ê°€ëŠ¥í•©ë‹ˆë‹¤.")
    
    try:
        # ê³ ìœ  íŒŒì¼ ID ìƒì„±
        file_id = str(uuid.uuid4())
        
        # ì„ì‹œ íŒŒì¼ì— PDF ì €ì¥
        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as temp_file:
            content = await file.read()
            temp_file.write(content)
            temp_file_path = temp_file.name
        
        # PDF ë‚´ìš© ì¶”ì¶œ
        pdf, simple_texts, simple_images = await extract_pdf_content(temp_file_path)
        
        # ë¬¸ì„œëª… ì„¤ì •
        clean_filename = file.filename.replace('.pdf', '').replace('.PDF', '')
        
        # íŒŒì¼ë³„ ëª¨ë¸ ìƒì„± ë° í…ìŠ¤íŠ¸ ì²­í‚¹
        file_model = await create_file_model(file_id, pdf, simple_texts, simple_images, clean_filename)
        
        # íŒŒì¼ë³„ ëª¨ë¸ ì €ì¥
        file_models[file_id] = file_model
        
        # íŒŒì¼ ì •ë³´ ì €ì¥
        text_chunks_count = len(file_model.chunks)
        file_info = {
            "id": file_id,
            "filename": file.filename,
            "document_name": clean_filename,
            "size": len(content),
            "content_type": file.content_type or "application/pdf",
            "model": "vqa_rag_client",
            "pages": len(pdf),
            "text_chunks": text_chunks_count,
            "image_chunks": 0,
            "total_chunks": text_chunks_count,
            "processed_at": datetime.now().isoformat(),
            "temp_file_path": temp_file_path,
            "image_processing_status": "pending"
        }
        
        uploaded_files[file_id] = file_info
        
        # ì´ë¯¸ì§€ ë°±ê·¸ë¼ìš´ë“œ ì²˜ë¦¬
        if simple_images:
            async def process_images_background():
                """ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì´ë¯¸ì§€ ë°°ì¹˜ ì²˜ë¦¬"""
                try:
                    # ì´ë¯¸ì§€ ì„¤ì •
                    processed_images = []
                    for img_tuple in simple_images:
                        if isinstance(img_tuple, tuple) and len(img_tuple) >= 3:
                            page_num, img_num, pil_img = img_tuple
                            if hasattr(pil_img, 'format'):
                                processed_images.append(pil_img)
                    
                    # íŒŒì¼ ëª¨ë¸ì— ì´ë¯¸ì§€ ì¶”ê°€
                    if file_id in file_models:
                        file_models[file_id].images = processed_images
                        
                        # ë°°ì¹˜ ë‹¨ìœ„ ì´ë¯¸ì§€ ì²˜ë¦¬
                        if processed_images:
                            # íŒŒì¼ ì •ë³´ì— ì§„í–‰ ìƒíƒœ ì¶”ê°€
                            if file_id in uploaded_files:
                                uploaded_files[file_id]["image_processing_status"] = "processing"
                                uploaded_files[file_id]["image_progress"] = 0
                                uploaded_files[file_id]["total_images"] = len(processed_images)
                            
                            # ì§„í–‰ ìƒíƒœ ì¶”ì ì„ ìœ„í•œ ì½œë°± í•¨ìˆ˜
                            async def progress_callback(batch_num, total_batches, processed_count, total_count):
                                progress = int((processed_count / total_count) * 100)
                                if file_id in uploaded_files:
                                    uploaded_files[file_id]["image_progress"] = progress
                                    uploaded_files[file_id]["processed_images"] = processed_count
                                print(f"ğŸ“Š ì´ë¯¸ì§€ ì²˜ë¦¬ ì§„í–‰ë¥ : {progress}% ({processed_count}/{total_count})")
                            
                            # ë°°ì¹˜ ì²˜ë¦¬ ì‹¤í–‰ (ë°°ì¹˜ í¬ê¸° 2)
                            await file_models[file_id].process_images_async(batch_size=2)
                            
                            # íŒŒì¼ ì •ë³´ ì—…ë°ì´íŠ¸
                            if file_id in uploaded_files:
                                image_chunks_count = len(getattr(file_models[file_id], 'image_chunks', []))
                                uploaded_files[file_id]["image_chunks"] = image_chunks_count
                                uploaded_files[file_id]["total_chunks"] = text_chunks_count + image_chunks_count
                                uploaded_files[file_id]["image_processing_status"] = "completed"
                                uploaded_files[file_id]["image_progress"] = 100
                                uploaded_files[file_id]["processed_images"] = len(processed_images)
                                
                                print(f"âœ… ë°±ê·¸ë¼ìš´ë“œ: ì´ë¯¸ì§€ ì²˜ë¦¬ ì™„ë£Œ - {image_chunks_count}ê°œ ì´ë¯¸ì§€ ì²­í¬")
                        else:
                            if file_id in uploaded_files:
                                uploaded_files[file_id]["image_processing_status"] = "no_images"
                    
                except Exception as e:
                    if file_id in uploaded_files:
                        uploaded_files[file_id]["image_processing_status"] = "error"
                        uploaded_files[file_id]["image_error"] = str(e)
            
            # ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ ìƒì„±
            asyncio.create_task(process_images_background())
        else:
            # ì´ë¯¸ì§€ê°€ ì—†ëŠ” ê²½ìš°
            if file_id in uploaded_files:
                uploaded_files[file_id]["image_processing_status"] = "no_images"
                uploaded_files[file_id]["image_progress"] = 100
        
        # OpenWebUI í‘œì¤€ í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ
        current_time = int(time.time())
        
        return FileUploadResponse(
            id=file_id,
            user_id="system",
            hash=None,
            filename=file.filename,
            data={
                "total_pages": len(pdf),
                "total_chunks": text_chunks_count,
                "text_chunks": text_chunks_count,
                "image_chunks": 0,
                "processing_status": "text_completed",
                "image_processing_status": "pending",
                "document_name": clean_filename
            },
            meta={
                "name": file.filename,
                "content_type": file.content_type or "application/pdf",
                "size": len(content),
                "data": {
                    "total_pages": len(pdf),
                    "total_chunks": text_chunks_count,
                    "text_chunks": text_chunks_count,
                    "image_chunks": 0,
                    "processing_status": "text_completed",
                    "image_processing_status": "pending",
                    "document_name": clean_filename
                }
            },
            created_at=current_time,
            updated_at=current_time
        )
        
    except Exception as e:
        if 'temp_file_path' in locals():
            os.unlink(temp_file_path)
        raise HTTPException(status_code=500, detail=f"íŒŒì¼ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")

@app.post("/v1/chat/completions")
async def chat_completions(request: ChatRequest):
    """OpenWebUI í˜¸í™˜ ì±„íŒ… ì™„ë£Œ API"""
    return await _chat_completions_logic(request)

@app.post("/api/chat/completions")
async def chat_completions_api(request: ChatRequest, api_key: str = Depends(verify_api_key)):
    """OpenWebUI í‘œì¤€ ì±„íŒ… ì™„ë£Œ API"""
    return await _chat_completions_logic(request)

@app.post("/api/v1/chat/completions")
async def chat_completions_v1(request: ChatRequest, api_key: str = Depends(verify_api_key)):
    """OpenWebUI v1 ì±„íŒ… ì™„ë£Œ API"""
    return await _chat_completions_logic(request)

async def _chat_completions_logic(request: ChatRequest):
    """ì±„íŒ… ì™„ë£Œ ê³µí†µ ë¡œì§ - ëª¨ë¸ ì„œë²„ í´ë¼ì´ì–¸íŠ¸ ì‚¬ìš©"""
    global file_models, uploaded_files, model_client
    
    # ì´ë¯¸ì§€ ì…ë ¥ bytesë¡œ ì²˜ë¦¬
    user_message = None
    image_bytes_list = []
    image_urls_from_content = []
    
    for msg in reversed(request.messages):
        content = msg.content
        # contentê°€ ë¦¬ìŠ¤íŠ¸(ë³µí•© ë©”ì‹œì§€)ì¸ ê²½ìš°
        if isinstance(content, list):
            texts = []
            for item in content:
                if isinstance(item, dict):
                    if item.get("type") == "text":
                        texts.append(item.get("text", ""))
                    elif item.get("type") == "image_url" and "image_url" in item:
                        image_url = item["image_url"]
                        if isinstance(image_url, dict) and "url" in image_url:
                            url = image_url["url"]
                            if url.startswith("data:image/"):
                                import re, base64
                                match = re.match(r"data:image/[^;]+;base64,(.*)", url)
                                if match:
                                    img_bytes = base64.b64decode(match.group(1))
                                    image_bytes_list.append(img_bytes)
                            else:
                                image_urls_from_content.append(url)
                        elif isinstance(image_url, str):
                            if image_url.startswith("data:image/"):
                                import re, base64
                                match = re.match(r"data:image/[^;]+;base64,(.*)", image_url)
                                if match:
                                    img_bytes = base64.b64decode(match.group(1))
                                    image_bytes_list.append(img_bytes)
                            else:
                                image_urls_from_content.append(image_url)
            if texts:
                user_message = "\n".join(texts)
            break
        elif isinstance(content, str) and content.strip():
            user_message = content.strip()
            break
    
    # request.imagesë„ bytesë¡œ ì²˜ë¦¬
    if hasattr(request, 'images') and request.images:
        for img in request.images:
            if isinstance(img, bytes):
                image_bytes_list.append(img)
            elif isinstance(img, str):
                import base64
                try:
                    image_bytes_list.append(base64.b64decode(img))
                except Exception:
                    pass
            elif isinstance(img, dict) and 'data' in img:
                import base64
                try:
                    image_bytes_list.append(base64.b64decode(img['data']))
                except Exception:
                    pass
    
    # image_urlsë„ ì²˜ë¦¬
    if hasattr(request, 'image_urls') and request.image_urls:
        for img_url_dict in request.image_urls:
            url = img_url_dict.get('url')
            if url:
                image_urls_from_content.append(url)
    
    # ì´ë¯¸ì§€ë§Œ ìˆì„ ë•Œ VQA
    if not uploaded_files and image_bytes_list:
        try:
            from PIL import Image
            from io import BytesIO
            img = Image.open(BytesIO(image_bytes_list[0]))
            prompt = user_message if user_message else "ì´ ì´ë¯¸ì§€ëŠ” ë¬´ì—‡ì¸ê°€ìš”?"
            answer = model_client.generate_vqa(prompt, img)
        except Exception as e:
            answer = "ì´ë¯¸ì§€ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        
        response = ChatResponse(
            id=f"chatcmpl-{str(uuid.uuid4())}",
            object="chat.completion",
            created=int(datetime.now().timestamp()),
            model=request.model,
            choices=[{
                "index": 0,
                "message": {"role": "assistant", "content": answer},
                "finish_reason": "stop"
            }],
            usage={
                "prompt_tokens": len(prompt.split()) if prompt else 0,
                "completion_tokens": len(answer.split()) if answer else 0,
                "total_tokens": (len(prompt.split()) if prompt else 0) + (len(answer.split()) if answer else 0)
            }
        )
        
        if request.stream:
            return create_streaming_response(answer, request.model)
        
        from fastapi.responses import JSONResponse
        return JSONResponse(content=response.dict())
    
    # í…ìŠ¤íŠ¸+ì´ë¯¸ì§€ í˜¼í•© ì‹œ ì´ë¯¸ì§€ ìš”ì•½
    image_summaries = []
    if image_bytes_list:
        from PIL import Image
        from io import BytesIO
        file_model = next(iter(file_models.values())) if file_models else None
        for idx, img_bytes in enumerate(image_bytes_list):
            try:
                img = Image.open(BytesIO(img_bytes))
                if file_model:
                    summary = file_model.summarize_image(img)
                    if summary:
                        image_summaries.append(f"ğŸ–¼ï¸ ì²¨ë¶€ ì´ë¯¸ì§€ {idx+1}: {summary}")
            except Exception as e:
                pass
    
    # image_urls ì²˜ë¦¬
    if image_urls_from_content:
        from PIL import Image
        from io import BytesIO
        file_model = next(iter(file_models.values())) if file_models else None
        for idx, url in enumerate(image_urls_from_content):
            try:
                resp = req.get(url)
                img = Image.open(BytesIO(resp.content))
                if file_model:
                    summary = file_model.summarize_image(img)
                    if summary:
                        image_summaries.append(f"ğŸ–¼ï¸ ì²¨ë¶€ ì´ë¯¸ì§€ URL {idx+1}: {summary}")
            except Exception as e:
                pass
    
    # ëª¨ë¸ ì„œë²„ ì—°ê²° í™•ì¸
    if not model_client or not model_client.is_healthy():
        raise HTTPException(status_code=500, detail="ëª¨ë¸ ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëª¨ë¸ ì„œë²„ë¥¼ ì‹œì‘í•´ì£¼ì„¸ìš”.")
    
    try:
        # system_messageë„ ì¶”ì¶œ
        system_message = None
        for msg in request.messages:
            if hasattr(msg, "role") and msg.role == "system" and msg.content:
                system_message = msg.content
                break
        
        # OpenWebUI ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì²˜ë¦¬
        if not user_message:
            for msg in request.messages:
                if msg.content and any(keyword in msg.content for keyword in ["### Task:", "Generate a concise", "title", "JSON format"]):
                    if "search queries" in msg.content.lower():
                        answer = "No search queries needed."
                    elif "title" in msg.content.lower():
                        if uploaded_files:
                            answer = f'{{"title": "ğŸ“„ ë¬¸ì„œ ë¶„ì„"}}'
                        else:
                            answer = '{"title": "ğŸ’¬ ì¼ë°˜ ëŒ€í™”"}'
                    elif "tags" in msg.content.lower():
                        if uploaded_files:
                            answer = '{"tags": ["Document", "PDF", "Analysis", "RAG"]}'
                        else:
                            answer = '{"tags": ["General", "Conversation"]}'
                    else:
                        answer = "Task completed."
                    
                    if request.stream:
                        return create_streaming_response(answer, request.model)
                    
                    response = ChatResponse(
                        id=f"chatcmpl-{str(uuid.uuid4())}",
                        object="chat.completion",
                        created=int(datetime.now().timestamp()),
                        model=request.model,
                        choices=[{
                            "index": 0,
                            "message": {"role": "assistant", "content": answer},
                            "finish_reason": "stop"
                        }],
                        usage={
                            "prompt_tokens": len(msg.content.split()) if msg.content else 0,
                            "completion_tokens": len(answer.split()) if answer else 0,
                            "total_tokens": (len(msg.content.split()) if msg.content else 0) + (len(answer.split()) if answer else 0)
                        }
                    )
                    from fastapi.responses import JSONResponse
                    return JSONResponse(content=response.dict())
            else:
                # í™˜ì˜ ë©”ì‹œì§€
                if uploaded_files:
                    file_list = []
                    for file_id, file_info in uploaded_files.items():
                        file_list.append(f"ğŸ“„ **ë¬¸ì„œ** ({file_info['total_chunks']}ê°œ ì²­í¬)")
                    
                    answer = f"""ì•ˆë…•í•˜ì„¸ìš”! í˜„ì¬ ì—…ë¡œë“œëœ ë¬¸ì„œê°€ ìˆìŠµë‹ˆë‹¤:

{chr(10).join(file_list)}

ë¬¸ì„œì— ëŒ€í•´ ì§ˆë¬¸í•´ì£¼ì„¸ìš”! ì˜ˆë¥¼ ë“¤ì–´:
- "ì´ ë¬¸ì„œì˜ ì£¼ìš” ë‚´ìš©ì€ ë¬´ì—‡ì¸ê°€ìš”?"
- "ì¤‘ìš”í•œ ì •ë³´ë¥¼ ìš”ì•½í•´ì£¼ì„¸ìš”" """
                else:
                    answer = "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”? PDF ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì‹œë©´ ë¬¸ì„œ ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        else:
            # ë””ë²„ê¹… ì•”í˜¸ë¬¸ ì²´í¬
            if detect_debug_command(user_message):
                answer = print_chunk_debug(file_models, uploaded_files)
            
            # ì‹œìŠ¤í…œ ë©”ì‹œì§€ì—ì„œ RAG ì»¨í…ìŠ¤íŠ¸ ì¶”ì¶œ
            elif system_message and "<context>" in system_message:
                import re
                context_match = re.search(r'<context>(.*?)</context>', system_message, re.DOTALL)
                if context_match:
                    rag_context = context_match.group(1).strip()
                    
                    # ì˜ë¯¸ì—†ëŠ” ì»¨í…ìŠ¤íŠ¸ ì²´í¬
                    useless_keywords = [
                        "ë¬¸ì„œ ì²˜ë¦¬ ì™„ë£Œ",
                        "ë¬¸ì„œê°€ ì„±ê³µì ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤",
                        "ì²˜ë¦¬ ìƒíƒœ:",
                        "í…ìŠ¤íŠ¸ ì²­í¬:",
                        "ì´ í˜ì´ì§€:",
                        "ë¬¸ì„œ ë‚´ìš©ì— ëŒ€í•´ ì§ˆë¬¸í•´ì£¼ì„¸ìš”"
                    ]
                    
                    is_useless_context = any(keyword in rag_context for keyword in useless_keywords)
                    
                    if is_useless_context:
                        answer = None  # íŒŒì¼ ê¸°ë°˜ RAGë¡œ ì „í™˜
                    else:
                        rag_prompt = f"""ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸:
{rag_context}

ì§ˆë¬¸: {user_message}

ë‹µë³€:"""
                        answer = model_client.generate_text(rag_prompt)
                else:
                    answer = "ì£„ì†¡í•©ë‹ˆë‹¤. ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
            # ì—…ë¡œë“œëœ íŒŒì¼ì´ ìˆìœ¼ë©´ íŒŒì¼ ê¸°ë°˜ RAG ì‚¬ìš©
            if uploaded_files and (answer is None or not answer):
                if len(file_models) > 1:
                    # ë‹¤ì¤‘ ë¬¸ì„œ ëª¨ë“œ
                    best_file_id = None
                    best_score = -float('inf')
                    for file_id, file_model in file_models.items():
                        results = file_model.retrieve(user_message, top_k=10)
                        score_sum = sum([score for _, _, score in results])
                        if score_sum > best_score:
                            best_score = score_sum
                            best_file_id = file_id
                    
                    if best_file_id is not None:
                        file_model = file_models[best_file_id]
                        
                        # ì´ë¯¸ì§€ ìš”ì•½ë„ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€
                        if image_summaries:
                            file_model.chunks = image_summaries + file_model.chunks
                        
                        answer = file_model.QnA(user_message, top_k=5)
                        answer = f"ğŸ“š **ë‹¤ì¤‘ ë¬¸ì„œ ê²€ìƒ‰** (ë¬¸ì„œ: {file_model.document_name})\n\n{answer}\n\n---\nğŸ“Š **ì´ ë¬¸ì„œ**: {len(file_models)}ê°œ"
                    else:
                        answer = "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
                else:
                    # ë‹¨ì¼ ë¬¸ì„œ ëª¨ë“œ
                    file_id = list(file_models.keys())[0]
                    file_model = file_models[file_id]
                    file_info = uploaded_files[file_id]
                    
                    if file_model.chunks:
                        # ì´ë¯¸ì§€ ìš”ì•½ë„ ì»¨í…ìŠ¤íŠ¸ì— ì¶”ê°€
                        if image_summaries:
                            file_model.chunks = image_summaries + file_model.chunks
                        
                        answer = file_model.QnA(user_message, top_k=5)
                        answer = f"ğŸ“„ ë¬¸ì„œ ê¸°ë°˜ ë‹µë³€:\n\n{answer}\n\n---\nğŸ“Š **ë¬¸ì„œ ì •ë³´**: {file_info['pages']}í˜ì´ì§€, {file_info['total_chunks']}ê°œ ì²­í¬"
                    else:
                        answer = "ì£„ì†¡í•©ë‹ˆë‹¤. íŒŒì¼ì´ ì•„ì§ ì²˜ë¦¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
            else:
                # ì¼ë°˜ ëŒ€í™” ëª¨ë“œ
                if not uploaded_files:
                    prompt = f"ì§ˆë¬¸: {user_message}\në‹µë³€ì„ í•´ì£¼ì„¸ìš”."
                    answer = model_client.generate_text(prompt)
                else:
                    answer = "ì£„ì†¡í•©ë‹ˆë‹¤. ì—…ë¡œë“œëœ ë¬¸ì„œê°€ ì•„ì§ ì²˜ë¦¬ ì¤‘ì´ê±°ë‚˜ ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤."
        
        # ë‹µë³€ ê²€ì¦
        if not answer or not answer.strip():
            answer = "ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ì„ ì´í•´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‹¤ë¥¸ ë°©ì‹ìœ¼ë¡œ ì§ˆë¬¸í•´ì£¼ì„¸ìš”."
        
        # ìŠ¤íŠ¸ë¦¬ë° ìš”ì²­ ì²˜ë¦¬
        if request.stream:
            return create_streaming_response(answer, request.model)
        
        # ì¼ë°˜ ì‘ë‹µ
        response = ChatResponse(
            id=f"chatcmpl-{str(uuid.uuid4())}",
            object="chat.completion",
            created=int(datetime.now().timestamp()),
            model=request.model,
            choices=[{
                "index": 0,
                "message": {"role": "assistant", "content": answer},
                "finish_reason": "stop"
            }],
            usage={
                "prompt_tokens": len(user_message.split()) if user_message else 0,
                "completion_tokens": len(answer.split()) if answer else 0,
                "total_tokens": (len(user_message.split()) if user_message else 0) + (len(answer.split()) if answer else 0)
            }
        )
        
        from fastapi.responses import JSONResponse
        return JSONResponse(content=response.dict())
        
    except Exception as e:
        print(f"ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {str(e)}")
        raise HTTPException(status_code=500, detail=f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {str(e)}")

def create_streaming_response(answer: str, request_model: str):
    """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±"""
    async def generate():
        chunk_id = f"chatcmpl-{str(uuid.uuid4())}"
        created = int(datetime.now().timestamp())
        
        # ì²« ë²ˆì§¸ ì²­í¬
        first_chunk = {
            "id": chunk_id,
            "object": "chat.completion.chunk",
            "created": created,
            "model": request_model,
            "choices": [{
                "index": 0,
                "delta": {"role": "assistant", "content": ""},
                "finish_reason": None
            }]
        }
        yield f"data: {json.dumps(first_chunk, ensure_ascii=False)}\n\n"
        
        # ë‹µë³€ì„ ì²­í¬ë¡œ ë‚˜ëˆ„ì–´ ì „ì†¡
        words = answer.split()
        chunk_size = 3
        
        for i in range(0, len(words), chunk_size):
            chunk_words = words[i:i+chunk_size]
            content = " ".join(chunk_words)
            if i > 0:
                content = " " + content
                
            chunk = {
                "id": chunk_id,
                "object": "chat.completion.chunk",
                "created": created,
                "model": request_model,
                "choices": [{
                    "index": 0,
                    "delta": {"content": content},
                    "finish_reason": None
                }]
            }
            yield f"data: {json.dumps(chunk, ensure_ascii=False)}\n\n"
            await asyncio.sleep(0.05)
        
        # ë§ˆì§€ë§‰ ì²­í¬
        final_chunk = {
            "id": chunk_id,
            "object": "chat.completion.chunk",
            "created": created,
            "model": request_model,
            "choices": [{
                "index": 0,
                "delta": {},
                "finish_reason": "stop"
            }]
        }
        yield f"data: {json.dumps(final_chunk, ensure_ascii=False)}\n\n"
        yield "data: [DONE]\n\n"
    
    return StreamingResponse(
        generate(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "X-Accel-Buffering": "no"
        }
    )

@app.get("/v1/status")
async def get_status():
    """í˜„ì¬ ìƒíƒœ í™•ì¸ - ì´ë¯¸ì§€ ì²˜ë¦¬ ì§„í–‰ ìƒí™© í¬í•¨"""
    model_server_status = "disconnected"
    if model_client:
        model_server_status = "connected" if model_client.is_healthy() else "unhealthy"
    
    # íŒŒì¼ë³„ ìƒì„¸ ì •ë³´ êµ¬ì„±
    files_info = {}
    for file_id, info in uploaded_files.items():
        file_detail = {
            "filename": info["filename"],
            "size": info["size"],
            "total_chunks": info["total_chunks"],
            "text_chunks": info.get("text_chunks", 0),
            "image_chunks": info.get("image_chunks", 0),
            "processed_at": info["processed_at"],
            "has_model": file_id in file_models,
            "image_processing_status": info.get("image_processing_status", "unknown"),
            "image_progress": info.get("image_progress", 0),
            "total_images": info.get("total_images", 0),
            "processed_images": info.get("processed_images", 0)
        }
        
        if "image_error" in info:
            file_detail["image_error"] = info["image_error"]
        
        files_info[file_id] = file_detail
    
    return {
        "model_server_status": model_server_status,
        "uploaded_files": len(uploaded_files),
        "file_models": len(file_models),
        "files_info": files_info,
        "processing_summary": {
            "total_files": len(uploaded_files),
            "completed_text_processing": len([f for f in uploaded_files.values() if f.get("text_chunks", 0) > 0]),
            "completed_image_processing": len([f for f in uploaded_files.values() if f.get("image_processing_status") == "completed"]),
            "processing_images": len([f for f in uploaded_files.values() if f.get("image_processing_status") == "processing"]),
            "total_text_chunks": sum(f.get("text_chunks", 0) for f in uploaded_files.values()),
            "total_image_chunks": sum(f.get("image_chunks", 0) for f in uploaded_files.values()),
            "total_chunks": sum(f.get("total_chunks", 0) for f in uploaded_files.values())
        }
    }

@app.delete("/api/v1/files/{file_id}")
async def delete_file(file_id: str):
    """íŒŒì¼ ì‚­ì œ"""
    global file_models, uploaded_files
    
    try:
        if file_id in uploaded_files:
            file_info = uploaded_files[file_id]
            
            if 'temp_file_path' in file_info and os.path.exists(file_info['temp_file_path']):
                os.unlink(file_info['temp_file_path'])
            
            del uploaded_files[file_id]
        
        if file_id in file_models:
            del file_models[file_id]
        
        return {
            "message": f"íŒŒì¼ {file_id}ê°€ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œë˜ì—ˆìŠµë‹ˆë‹¤.",
            "deleted_file_id": file_id,
            "remaining_files": len(uploaded_files),
            "remaining_models": len(file_models)
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"íŒŒì¼ ì‚­ì œ ì¤‘ ì˜¤ë¥˜: {str(e)}")

@app.delete("/v1/files/{file_id}")
async def delete_file_v1(file_id: str):
    """OpenAI í˜¸í™˜ íŒŒì¼ ì‚­ì œ ì—”ë“œí¬ì¸íŠ¸"""
    return await delete_file(file_id)

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000) 